# 2-hidden-layer-NN

Implementation of a 2-hidden-layer fully connected neural network from scratch using python

The aim of this project was to fully understand the calculus behind the forward and backpropagation algorithms.

The network has ReLU functions for the activation on its hidden layers and a sigmoid activation function on the last layer.

It was implemented for binary classification problems using the 